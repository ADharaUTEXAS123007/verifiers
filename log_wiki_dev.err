
Lmod is automatically replacing "nvidia/24.7" with "gcc/13.2.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) openmpi/5.0.5

The following have been reloaded with a version change:
  1) nvpl/24.7 => nvpl/25.9


Due to MODULEPATH changes, the following have been reloaded:
  1) openmpi/5.0.5

DeprecationWarning: 'source deactivate' is deprecated. Use 'conda deactivate'.
/scratch/09143/arnabd/newproj/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2026-01-23 04:47:04 - verifiers.utils.install_utils - INFO - Installing wiki-search from environments/wiki_search...
/scratch/09143/arnabd/newproj/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2026-01-23 04:47:09 - verifiers.utils.install_utils - INFO - Successfully installed wiki-search
[W123 04:47:10.908492997 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
2026-01-23 04:47:22 - verifiers.utils.env_utils - INFO - Loading environment: wiki-search
2026-01-23 04:47:26 - verifiers.utils.env_utils - INFO - Using provided args: max_turns=10
2026-01-23 04:47:26 - verifiers.utils.env_utils - INFO - Using default args: corpus_split='train', corpus_dataset='willcb/rare-wiki-pages', embed_api_key_var='OPENAI_API_KEY', judge_base_url='https://api.openai.com/v1', embed_model='text-embedding-3-small', chroma_db_dir='.chroma_db', judge_api_key_var='OPENAI_API_KEY', judge_model='gpt-4.1-mini', embed_base_url='https://api.openai.com/v1'
[0;36m(EngineCore_DP0 pid=1071989)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
2026-01-23 04:47:32 - verifiers.utils.env_utils - INFO - Successfully loaded environment 'wiki-search'
[0;36m(EngineCore_DP0 pid=1071989)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:07<00:15,  7.63s/it]
/scratch/09143/arnabd/newproj/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[0;36m(EngineCore_DP0 pid=1071989)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:15<00:07,  7.81s/it]
[0;36m(EngineCore_DP0 pid=1071989)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:15<00:00,  4.35s/it]
[0;36m(EngineCore_DP0 pid=1071989)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:15<00:00,  5.27s/it]
[0;36m(EngineCore_DP0 pid=1071989)[0;0m 
INFO:     Started server process [1071911]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  3.11it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  2.99it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.45it/s]
2026-01-23 04:48:25 - verifiers.rl.inference.client - INFO - Server is up!
2026-01-23 04:48:25 - verifiers.rl.inference.client - INFO - vLLM world size: 1
2026-01-23 04:48:25 - verifiers.rl.inference.client - INFO - Client rank: 1, total world size: 2
2026-01-23 04:48:25 - verifiers.rl.inference.client - INFO - Initializing PyNcclCommunicator on device 0, rank 1, world_size 2
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Processing 32 groups (512 total rollouts):   0%|          | 0/32 [00:00<?, ?it/s, reward=?]wandb: Currently logged in as: adhara (sparkcognition-services) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in /scratch/09143/arnabd/newproj/wandb/wandb/run-20260123_044827-t9hlx2wg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wiki-search
wandb: â­ï¸ View project at https://wandb.ai/sparkcognition-services/wiki-search
wandb: ðŸš€ View run at https://wandb.ai/sparkcognition-services/wiki-search/runs/t9hlx2wg
wandb: Detected [mcp, openai, agents, verifiers] in use.
wandb: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/

  0%|          | 0/500 [00:00<?, ?it/s][A2026-01-23 04:48:35 - verifiers.rl.trainer.trainer - INFO - Waiting for generation to finish before syncing.
2026-01-23 04:48:40 - verifiers.rl.trainer.trainer - INFO - Waiting for generation to finish before syncing.
2026-01-23 04:48:45 - verifiers.rl.trainer.trainer - INFO - Waiting for generation to finish before syncing.
2026-01-23 04:48:51 - verifiers.rl.trainer.trainer - INFO - Waiting for generation to finish before syncing.
Processing 32 groups (512 total rollouts):   3%|â–Ž         | 1/32 [00:25<13:21, 25.85s/it, reward=?]Processing 32 groups (512 total rollouts):   3%|â–Ž         | 1/32 [00:25<13:21, 25.85s/it, reward=0.062]Processing 32 groups (512 total rollouts):   6%|â–‹         | 2/32 [00:25<12:55, 25.85s/it, reward=0.156]Processing 32 groups (512 total rollouts):   9%|â–‰         | 3/32 [00:26<03:26,  7.13s/it, reward=0.156]Processing 32 groups (512 total rollouts):   9%|â–‰         | 3/32 [00:26<03:26,  7.13s/it, reward=0.438]Processing 32 groups (512 total rollouts):  12%|â–ˆâ–Ž        | 4/32 [00:27<02:19,  4.97s/it, reward=0.438]Processing 32 groups (512 total rollouts):  12%|â–ˆâ–Ž        | 4/32 [00:27<02:19,  4.97s/it, reward=0.328]2026-01-23 04:48:56 - verifiers.rl.trainer.trainer - INFO - Waiting for generation to finish before syncing.
2026-01-23 04:49:01 - verifiers.rl.trainer.trainer - INFO - Waiting for generation to finish before syncing.
Processing 32 groups (512 total rollouts):  16%|â–ˆâ–Œ        | 5/32 [00:36<02:48,  6.23s/it, reward=0.328]Processing 32 groups (512 total rollouts):  16%|â–ˆâ–Œ        | 5/32 [00:36<02:48,  6.23s/it, reward=0.263]2026-01-23 04:49:06 - verifiers.rl.trainer.trainer - INFO - Waiting for generation to finish before syncing.
Processing 32 groups (512 total rollouts):  19%|â–ˆâ–‰        | 6/32 [00:43<02:47,  6.45s/it, reward=0.263]Processing 32 groups (512 total rollouts):  19%|â–ˆâ–‰        | 6/32 [00:43<02:47,  6.45s/it, reward=0.219]Processing 32 groups (512 total rollouts):  22%|â–ˆâ–ˆâ–       | 7/32 [00:43<02:41,  6.45s/it, reward=0.330]Processing 32 groups (512 total rollouts):  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:43<02:34,  6.45s/it, reward=0.414]Processing 32 groups (512 total rollouts):  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:43<01:03,  2.76s/it, reward=0.414]Processing 32 groups (512 total rollouts):  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:43<01:03,  2.76s/it, reward=0.472]Processing 32 groups (512 total rollouts):  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:43<01:00,  2.76s/it, reward=0.525]Processing 32 groups (512 total rollouts):  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:44<00:38,  1.84s/it, reward=0.525]Processing 32 groups (512 total rollouts):  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:44<00:38,  1.84s/it, reward=0.568]Processing 32 groups (512 total rollouts):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:44<00:31,  1.56s/it, reward=0.568]Processing 32 groups (512 total rollouts):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:44<00:31,  1.56s/it, reward=0.526]2026-01-23 04:49:11 - verifiers.rl.trainer.trainer - INFO - Waiting for generation to finish before syncing.
Processing 32 groups (512 total rollouts):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:45<00:24,  1.31s/it, reward=0.526]Processing 32 groups (512 total rollouts):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:45<00:24,  1.31s/it, reward=0.558]Processing 32 groups (512 total rollouts):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:45<00:23,  1.31s/it, reward=0.589]Processing 32 groups (512 total rollouts):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:45<00:16,  1.05it/s, reward=0.589]Processing 32 groups (512 total rollouts):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:45<00:16,  1.05it/s, reward=0.617]Processing 32 groups (512 total rollouts):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:46<00:14,  1.14it/s, reward=0.617]Processing 32 groups (512 total rollouts):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:46<00:14,  1.14it/s, reward=0.641]Processing 32 groups (512 total rollouts):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:46<00:13,  1.14it/s, reward=0.603]Processing 32 groups (512 total rollouts):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:46<00:12,  1.14it/s, reward=0.573]Processing 32 groups (512 total rollouts):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:46<00:11,  1.14it/s, reward=0.595]Processing 32 groups (512 total rollouts):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:46<00:10,  1.14it/s, reward=0.566]Processing 32 groups (512 total rollouts):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:46<00:09,  1.14it/s, reward=0.539]Processing 32 groups (512 total rollouts):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:47<00:03,  2.64it/s, reward=0.539]Processing 32 groups (512 total rollouts):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:47<00:03,  2.64it/s, reward=0.514]Processing 32 groups (512 total rollouts):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:47<00:03,  2.64it/s, reward=0.495]Processing 32 groups (512 total rollouts):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:47<00:02,  2.86it/s, reward=0.495]Processing 32 groups (512 total rollouts):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:47<00:02,  2.86it/s, reward=0.474]Processing 32 groups (512 total rollouts):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:48<00:02,  2.85it/s, reward=0.474]Processing 32 groups (512 total rollouts):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:48<00:02,  2.85it/s, reward=0.458]Processing 32 groups (512 total rollouts):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:48<00:02,  2.58it/s, reward=0.458]Processing 32 groups (512 total rollouts):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:48<00:02,  2.58it/s, reward=0.445]Processing 32 groups (512 total rollouts):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:49<00:02,  2.15it/s, reward=0.445]Processing 32 groups (512 total rollouts):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:49<00:02,  2.15it/s, reward=0.456]Processing 32 groups (512 total rollouts):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:49<00:01,  2.15it/s, reward=0.475]Processing 32 groups (512 total rollouts):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:49<00:01,  2.15it/s, reward=0.470]2026-01-23 04:49:16 - verifiers.rl.trainer.trainer - INFO - Waiting for generation to finish before syncing.
Processing 32 groups (512 total rollouts):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:50<00:00,  2.24it/s, reward=0.470]Processing 32 groups (512 total rollouts):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:50<00:00,  2.24it/s, reward=0.454]Processing 32 groups (512 total rollouts):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:52<00:00,  1.45it/s, reward=0.454]Processing 32 groups (512 total rollouts):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:52<00:00,  1.45it/s, reward=0.440]2026-01-23 04:49:21 - verifiers.rl.trainer.trainer - INFO - Waiting for generation to finish before syncing.
Processing 32 groups (512 total rollouts): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:55<00:00,  1.11s/it, reward=0.440]Processing 32 groups (512 total rollouts): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:55<00:00,  1.11s/it, reward=0.426]Processing 32 groups (512 total rollouts): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:55<00:00,  1.73s/it, reward=0.426]
Processing 32 groups (512 total rollouts):   0%|          | 0/32 [00:00<?, ?it/s, reward=?]Processing 32 groups (512 total rollouts):   3%|â–Ž         | 1/32 [00:08<04:38,  8.98s/it, reward=?]Processing 32 groups (512 total rollouts):   3%|â–Ž         | 1/32 [00:08<04:38,  8.98s/it, reward=0.000]Processing 32 groups (512 total rollouts):   6%|â–‹         | 2/32 [00:27<07:19, 14.64s/it, reward=0.000]Processing 32 groups (512 total rollouts):   6%|â–‹         | 2/32 [00:27<07:19, 14.64s/it, reward=0.000]Processing 32 groups (512 total rollouts):   9%|â–‰         | 3/32 [00:33<05:14, 10.86s/it, reward=0.000]Processing 32 groups (512 total rollouts):   9%|â–‰         | 3/32 [00:33<05:14, 10.86s/it, reward=0.000]Processing 32 groups (512 total rollouts):  12%|â–ˆâ–Ž        | 4/32 [00:36<03:29,  7.49s/it, reward=0.000]Processing 32 groups (512 total rollouts):  12%|â–ˆâ–Ž        | 4/32 [00:36<03:29,  7.49s/it, reward=0.109]Processing 32 groups (512 total rollouts):  16%|â–ˆâ–Œ        | 5/32 [00:36<02:13,  4.96s/it, reward=0.109]Processing 32 groups (512 total rollouts):  16%|â–ˆâ–Œ        | 5/32 [00:36<02:13,  4.96s/it, reward=0.237]Processing 32 groups (512 total rollouts):  19%|â–ˆâ–‰        | 6/32 [00:36<01:26,  3.33s/it, reward=0.237]Processing 32 groups (512 total rollouts):  19%|â–ˆâ–‰        | 6/32 [00:36<01:26,  3.33s/it, reward=0.312]Processing 32 groups (512 total rollouts):  22%|â–ˆâ–ˆâ–       | 7/32 [00:37<01:00,  2.44s/it, reward=0.312]Processing 32 groups (512 total rollouts):  22%|â–ˆâ–ˆâ–       | 7/32 [00:37<01:00,  2.44s/it, reward=0.268]Processing 32 groups (512 total rollouts):  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:37<00:58,  2.44s/it, reward=0.359]Processing 32 groups (512 total rollouts):  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:37<00:56,  2.44s/it, reward=0.431]Processing 32 groups (512 total rollouts):  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:37<00:23,  1.05s/it, reward=0.431]Processing 32 groups (512 total rollouts):  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:37<00:23,  1.05s/it, reward=0.487]Processing 32 groups (512 total rollouts):  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:37<00:22,  1.05s/it, reward=0.534]Processing 32 groups (512 total rollouts):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:37<00:20,  1.05s/it, reward=0.552]Processing 32 groups (512 total rollouts):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:37<00:19,  1.05s/it, reward=0.510]Processing 32 groups (512 total rollouts):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:37<00:18,  1.05s/it, reward=0.473]Processing 32 groups (512 total rollouts):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:37<00:17,  1.05s/it, reward=0.500]Processing 32 groups (512 total rollouts):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:37<00:16,  1.05s/it, reward=0.492]Processing 32 groups (512 total rollouts):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:37<00:15,  1.05s/it, reward=0.522]Processing 32 groups (512 total rollouts):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:37<00:04,  2.99it/s, reward=0.522]Processing 32 groups (512 total rollouts):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:37<00:04,  2.99it/s, reward=0.549]Processing 32 groups (512 total rollouts):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:37<00:04,  2.99it/s, reward=0.520]Processing 32 groups (512 total rollouts):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:37<00:04,  2.99it/s, reward=0.506]Processing 32 groups (512 total rollouts):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:37<00:03,  2.99it/s, reward=0.530]Processing 32 groups (512 total rollouts):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:37<00:03,  2.99it/s, reward=0.537]Processing 32 groups (512 total rollouts):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:37<00:03,  2.99it/s, reward=0.524]Processing 32 groups (512 total rollouts):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:37<00:02,  2.99it/s, reward=0.544]Processing 32 groups (512 total rollouts):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:37<00:01,  5.36it/s, reward=0.544]Processing 32 groups (512 total rollouts):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:37<00:01,  5.36it/s, reward=0.537]Processing 32 groups (512 total rollouts):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:37<00:01,  5.36it/s, reward=0.529]Processing 32 groups (512 total rollouts):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:37<00:00,  5.36it/s, reward=0.532]Processing 32 groups (512 total rollouts):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:38<00:00,  5.36it/s, reward=0.516]Processing 32 groups (512 total rollouts):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:38<00:00,  5.36it/s, reward=0.498]Processing 32 groups (512 total rollouts):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:38<00:00,  5.82it/s, reward=0.498]Processing 32 groups (512 total rollouts):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:38<00:00,  5.82it/s, reward=0.483]Processing 32 groups (512 total rollouts):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:46<00:00,  5.82it/s, reward=0.478]Processing 32 groups (512 total rollouts): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:47<00:00,  5.82it/s, reward=0.463]Processing 32 groups (512 total rollouts): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:47<00:00,  1.50s/it, reward=0.463]

  0%|          | 1/500 [05:10<43:04:35, 310.77s/it][A
                                                   [A
  0%|          | 1/500 [05:10<43:04:35, 310.77s/it][A2026-01-23 04:53:42 - verifiers.rl.trainer.trainer - INFO - Starting weight sync to vLLM
2026-01-23 04:53:44 - verifiers.rl.trainer.trainer - INFO - Resetting prefix cache.
Processing 32 groups (512 total rollouts):   0%|          | 0/32 [00:00<?, ?it/s, reward=?]Processing 32 groups (512 total rollouts):   3%|â–Ž         | 1/32 [00:11<05:41, 11.01s/it, reward=?]Processing 32 groups (512 total rollouts):   3%|â–Ž         | 1/32 [00:11<05:41, 11.01s/it, reward=0.000]Processing 32 groups (512 total rollouts):   6%|â–‹         | 2/32 [00:31<08:17, 16.59s/it, reward=0.000]Processing 32 groups (512 total rollouts):   6%|â–‹         | 2/32 [00:31<08:17, 16.59s/it, reward=0.500]Processing 32 groups (512 total rollouts):   9%|â–‰         | 3/32 [00:32<04:33,  9.42s/it, reward=0.500]Processing 32 groups (512 total rollouts):   9%|â–‰         | 3/32 [00:32<04:33,  9.42s/it, reward=0.417]Processing 32 groups (512 total rollouts):  12%|â–ˆâ–Ž        | 4/32 [00:32<04:23,  9.42s/it, reward=0.500]Processing 32 groups (512 total rollouts):  16%|â–ˆâ–Œ        | 5/32 [00:32<01:51,  4.13s/it, reward=0.500]Processing 32 groups (512 total rollouts):  16%|â–ˆâ–Œ        | 5/32 [00:32<01:51,  4.13s/it, reward=0.400]Processing 32 groups (512 total rollouts):  19%|â–ˆâ–‰        | 6/32 [00:33<01:20,  3.11s/it, reward=0.400]Processing 32 groups (512 total rollouts):  19%|â–ˆâ–‰        | 6/32 [00:33<01:20,  3.11s/it, reward=0.500]Processing 32 groups (512 total rollouts):  22%|â–ˆâ–ˆâ–       | 7/32 [00:34<01:05,  2.61s/it, reward=0.500]Processing 32 groups (512 total rollouts):  22%|â–ˆâ–ˆâ–       | 7/32 [00:34<01:05,  2.61s/it, reward=0.429]Processing 32 groups (512 total rollouts):  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:34<01:02,  2.61s/it, reward=0.375]Processing 32 groups (512 total rollouts):  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:36<00:41,  1.81s/it, reward=0.375]Processing 32 groups (512 total rollouts):  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:36<00:41,  1.81s/it, reward=0.438]Processing 32 groups (512 total rollouts):  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:36<00:32,  1.47s/it, reward=0.438]Processing 32 groups (512 total rollouts):  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:36<00:32,  1.47s/it, reward=0.494]Processing 32 groups (512 total rollouts):  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:36<00:23,  1.13s/it, reward=0.494]Processing 32 groups (512 total rollouts):  34%|â–ˆâ–ˆâ–ˆâ–      | 11/32 [00:36<00:23,  1.13s/it, reward=0.540]Processing 32 groups (512 total rollouts):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:36<00:22,  1.13s/it, reward=0.578]Processing 32 groups (512 total rollouts):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:37<00:13,  1.43it/s, reward=0.578]Processing 32 groups (512 total rollouts):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:37<00:13,  1.43it/s, reward=0.577]Processing 32 groups (512 total rollouts):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:37<00:12,  1.43it/s, reward=0.607]Processing 32 groups (512 total rollouts):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:37<00:11,  1.43it/s, reward=0.567]Processing 32 groups (512 total rollouts):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:37<00:11,  1.43it/s, reward=0.578]Processing 32 groups (512 total rollouts):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:37<00:10,  1.43it/s, reward=0.603]Processing 32 groups (512 total rollouts):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:37<00:09,  1.43it/s, reward=0.576]Processing 32 groups (512 total rollouts):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:37<00:09,  1.43it/s, reward=0.595]Processing 32 groups (512 total rollouts):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:37<00:08,  1.43it/s, reward=0.616]Processing 32 groups (512 total rollouts):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:37<00:07,  1.43it/s, reward=0.634]Processing 32 groups (512 total rollouts):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:37<00:06,  1.43it/s, reward=0.605]Processing 32 groups (512 total rollouts):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:37<00:06,  1.43it/s, reward=0.579]Processing 32 groups (512 total rollouts):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:37<00:01,  5.91it/s, reward=0.579]Processing 32 groups (512 total rollouts):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [00:37<00:01,  5.91it/s, reward=0.596]Processing 32 groups (512 total rollouts):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:38<00:01,  5.91it/s, reward=0.573]Processing 32 groups (512 total rollouts):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:38<00:01,  5.91it/s, reward=0.550]Processing 32 groups (512 total rollouts):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:38<00:00,  5.91it/s, reward=0.530]Processing 32 groups (512 total rollouts):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:39<00:01,  3.86it/s, reward=0.530]Processing 32 groups (512 total rollouts):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 28/32 [00:39<00:01,  3.86it/s, reward=0.511]Processing 32 groups (512 total rollouts):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:39<00:00,  3.86it/s, reward=0.494]Processing 32 groups (512 total rollouts):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:39<00:00,  3.86it/s, reward=0.477]Processing 32 groups (512 total rollouts):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:39<00:00,  3.96it/s, reward=0.477]Processing 32 groups (512 total rollouts):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 31/32 [00:39<00:00,  3.96it/s, reward=0.490]Processing 32 groups (512 total rollouts): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:40<00:00,  3.96it/s, reward=0.504]Processing 32 groups (512 total rollouts): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:40<00:00,  1.27s/it, reward=0.504]
Traceback (most recent call last):
  File "/scratch/09143/arnabd/newproj/.venv/bin/vf-train", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/09143/arnabd/newproj3/verifiers_mybranch/verifiers/verifiers/scripts/train.py", line 36, in main
    trainer.train()
  File "/scratch/09143/arnabd/newproj/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/09143/arnabd/newproj3/verifiers_mybranch/verifiers/verifiers/rl/trainer/trainer.py", line 397, in _inner_training_loop
    return super()._inner_training_loop(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/09143/arnabd/newproj/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/09143/arnabd/newproj3/verifiers_mybranch/verifiers/verifiers/rl/trainer/trainer.py", line 195, in training_step
    trainer_logprobs, entropies = self.get_logprobs(model, input_ids, attn_mask)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/09143/arnabd/newproj3/verifiers_mybranch/verifiers/verifiers/rl/trainer/trainer.py", line 297, in get_logprobs
    logits = model(
             ^^^^^^
  File "/scratch/09143/arnabd/newproj/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/09143/arnabd/newproj/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/09143/arnabd/newproj/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/09143/arnabd/newproj/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/09143/arnabd/newproj/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py", line 786, in convert_to_fp32
    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/09143/arnabd/newproj/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py", line 120, in recursively_apply
    k: recursively_apply(
       ^^^^^^^^^^^^^^^^^^
  File "/scratch/09143/arnabd/newproj/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py", line 127, in recursively_apply
    return func(data, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/09143/arnabd/newproj/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py", line 778, in _convert_to_fp32
    return tensor.float()
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.43 GiB. GPU 0 has a total capacity of 95.00 GiB of which 8.98 GiB is free. Including non-PyTorch memory, this process has 86.01 GiB memory in use. Of the allocated memory 81.40 GiB is allocated by PyTorch, and 1.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[W123 04:55:14.537243206 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
srun: error: c608-141: task 0: Exited with exit code 1
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: got SIGCONT
slurmstepd: error: *** JOB 547887 ON c608-132 CANCELLED AT 2026-01-23T05:59:42 ***
srun: forcing job termination
slurmstepd: error: *** STEP 547887.0 ON c608-132 CANCELLED AT 2026-01-23T05:59:42 ***
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
[rank0]:[W123 05:59:43.285840996 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[W123 05:59:43.995307584 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
